{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6537c249",
      "metadata": {
        "id": "6537c249"
      },
      "source": [
        "\n",
        "# Prompt Engineering Lab on Google AI Platform (Vertex AI)\n",
        "\n",
        "> **Note:** This lab should be done after the intro_prompt_design.ipynb where you will setup your VertexAI credentials.\n",
        "\n",
        "**First Colab**: <table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb#scrollTo=154137022fb6\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "bfcca6b1",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcca6b1",
        "outputId": "b92df410-6279-4624-f916-7560206a1477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.46.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U google-genai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38260b02",
      "metadata": {
        "id": "38260b02"
      },
      "source": [
        "\n",
        "### Environment configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "82536312",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82536312",
        "outputId": "c631b0d6-56af-4eb4-d21a-222a7e2dadf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex client ready: True\n",
            "Project: xenon-notch-443912-b5 Location: us-central1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from IPython.display import Markdown, display\n",
        "from google.genai.types import GenerateContentConfig\n",
        "\n",
        "# --- Set your project and region here ---\n",
        "PROJECT_ID = \"xenon-notch-443912-b5\"\n",
        "LOCATION   = \"us-central1\"\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "print(\"Vertex client ready:\", client is not None)\n",
        "print(\"Project:\", PROJECT_ID, \"Location:\", LOCATION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "574a65a8",
      "metadata": {
        "id": "574a65a8"
      },
      "source": [
        "\n",
        "### Choose a model\n",
        "\n",
        "We’ll use **Gemini 2.5** family.  \n",
        "- **`gemini-2.5-pro`**: best reasoning (higher cost/latency).  \n",
        "- **`gemini-2.5-flash`**: fast & economical (great for labs).\n",
        "\n",
        "We'll also define helpers for token counting and safe printing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "fede0fa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fede0fa2",
        "outputId": "0dd9707c-b4e4-4f9e-f900-cfbf5f9a81ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MODEL_FAST = \"gemini-2.5-flash\"\n",
        "MODEL_PRO  = \"gemini-2.5-pro\"\n",
        "\n",
        "def count_tokens(prompt_or_contents, model=MODEL_FAST):\n",
        "    \"\"\"Return token usage estimate for a given input.    Works with Vertex AI per google-genai docs.\"\"\"\n",
        "    resp = client.models.count_tokens(model=model, contents=prompt_or_contents)\n",
        "    return resp.total_tokens\n",
        "\n",
        "def pp(resp):\n",
        "    \"\"\"Pretty-print text from a generate_content response.\"\"\"\n",
        "    try:\n",
        "        print(resp.text)\n",
        "    except Exception as e:\n",
        "        print(\"No .text found, raw response:\")\n",
        "        print(resp)\n",
        "\n",
        "print(\"Models ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4397f24e",
      "metadata": {
        "id": "4397f24e"
      },
      "source": [
        "--------\n",
        "\n",
        "## 1. Prompt Pattern\n",
        "#### A- Persona Pattern : Act as persona X, Perform task Y\n",
        "Write a prompt example where you use the persona pattern to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "-e38wmjx7AlY"
      },
      "id": "-e38wmjx7AlY",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "fbe15b01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "fbe15b01",
        "outputId": "109b4967-723b-4917-b34a-6958cd6c4993"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Bonjour, lovely adventurers! Oh là là, what a wonderful question! While Paris certainly steals many hearts, France is absolutely bursting with other magnificent treasures just waiting to be discovered!\n\nIf you're asking me for *the* best place beyond the City of Lights, my dear friends, I simply *have* to point you towards the utterly enchanting region of **Provence**! ✨\n\nImagine this: rolling lavender fields stretching as far as the eye can see, charming hilltop villages straight out of a postcard, the scent of wild herbs in the air, and sunshine, glorious sunshine almost all year round! It's like stepping into a painting by Van Gogh or Cézanne himself!\n\nYou could wander through the historic Pope's Palace in Avignon, sip a delightful rosé in a sun-drenched café in Aix-en-Provence, explore the vibrant markets brimming with local produce and artisanal crafts, or get lost in the ochre-colored streets of Roussillon and the breathtaking views from Gordes. The food, the wine, the history, the incredible light – it's all just *magnifique*!\n\nIt's a place that truly soothes the soul and delights all your senses. Trust me, a trip to Provence will fill your heart with joy and leave you with memories as beautiful as its landscapes! *C'est parfait!* So, when are we going?!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Persona Pattern\n",
        "prompt=\"\"\"Act like cheerful tourist guide and answer the question\n",
        "\n",
        "question=Which is the best place to visit in France other than Paris?\n",
        "answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(model=MODEL_FAST, contents=prompt)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96d65cee",
      "metadata": {
        "id": "96d65cee"
      },
      "source": [
        "#### B- Audience Persona Pattern : Assume that I am Persona X, explain to me Y\n",
        "Write a prompt example where you use the Audience persona pattern to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "076b2ba1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "076b2ba1",
        "outputId": "95019a61-1971-498d-ffd1-0e0320823db6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Oh, that's a great question! For my next one, we're diving deep into an **action-thriller** with a strong emotional core. Think high stakes, intense sequences, and a story that'll keep you guessing until the very end. I can't wait for you all to see it!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Audience Persona Pattern\n",
        "prompt=\"\"\"Act as if you are talking to a group of fans\n",
        "\n",
        "example1:\n",
        "Question1:What is your next movie?\n",
        "Answer:My next movie is ABCD chapter 2\n",
        "\n",
        "example2:What are your current projects?\n",
        "Answer: As of now, I'm doing 2 movies in Malayalam, 1 in Hindi and 2 in Kannada.\n",
        "\n",
        "example3:What genre will be the next movie?\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(model=MODEL_FAST, contents=prompt)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ef0a41",
      "metadata": {
        "id": "d4ef0a41"
      },
      "source": [
        "#### C- Question Refinement Pattern : Ensure that LLM suggests potentially better or more refined questions the user could ask instead of original one.\n",
        "**Example**: Whenever I ask a question, suggest a better question and ask me if I would like to use it instead.\n",
        "What I can do next weekend to be happy?\n",
        "\n",
        "Write a prompt example where you use the Question Refinement Patternto the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "c9e1d781",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "c9e1d781",
        "outputId": "028de6fa-082e-4663-f6a5-bcda2bd3c8cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "That's a great question, and it highlights a common scenario where a slight rephrasing can make a big difference in how the other person responds!\n\nThe original question, \"Can we watch a movie today or would a concert be better?\" is perfectly understandable, but it can feel a little bit like a test or put the other person on the spot to pick the \"better\" option.\n\nHere are a few patterns that can make your questions more engaging, collaborative, and open to broader responses, applied to your example:\n\n---\n\n### **Suggested Question Patterns:**\n\n1.  **The Collaborative & Open-Ended Approach:**\n    *   **Pattern:** \"What are you in the mood for today – [Option A], [Option B], or something else?\"\n    *   **Example:** \"What are you in the mood for today – a movie, a concert, or something else entirely?\"\n    *   **Why it's better:** It's inclusive (\"what are *you* in the mood for\"), gives clear options, and explicitly invites other ideas, reducing the pressure to choose *only* between your suggestions.\n\n2.  **The Preference-Focused Approach:**\n    *   **Pattern:** \"Between [Option A] and [Option B], which sounds more appealing to you today?\"\n    *   **Example:** \"Between watching a movie and going to a concert, which sounds more appealing to you today?\"\n    *   **Why it's better:** It directly asks about their preference, making it about what *they* would enjoy rather than which option is objectively \"better.\"\n\n3.  **The Benefit-Oriented & Suggestive Approach:**\n    *   **Pattern:** \"I was thinking we could [describe Option A experience], or if you prefer, we could [describe Option B experience]. What do you think?\"\n    *   **Example:** \"I was thinking we could chill out with a movie today, or if you prefer, we could go for the excitement of a concert. What do you think?\"\n    *   **Why it's better:** It frames the options by their potential experiences/benefits, making them more attractive and giving the other person a clearer picture of what each entails. It also shows you've put some thought into it.\n\n4.  **The Direct & Flexible Approach:**\n    *   **Pattern:** \"For today, would you prefer [Option A], or are you leaning more towards [Option B]? I'm open to either!\"\n    *   **Example:** \"For today, would you prefer watching a movie, or are you leaning more towards a concert? I'm open to either!\"\n    *   **Why it's better:** It's clear, direct, uses \"prefer\" and \"leaning towards\" (softer than \"better\"), and crucially, your statement \"I'm open to either!\" makes it truly collaborative and less like you're pushing one option.\n\n---\n\n### **General Tips for Better Questioning:**\n\n*   **Be Specific:** (Your original question was good here with \"today.\")\n*   **Be Open-Ended:** Avoid questions that only lead to \"yes\" or \"no\" if you want more information.\n*   **Focus on \"You\":** Frame questions around the other person's preference, feelings, or needs.\n*   **Invite Alternatives:** Explicitly ask if they have other ideas.\n*   **Provide Context (if helpful):** Briefly explain *why* you're asking or why you suggested certain options.\n*   **Express Your Own Openness:** Let them know you're flexible.\n\n---\n\n**Would you like me to use one of these patterns, or a combination, whenever you ask a question in the future?** Please let me know your preference!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#TODO: Question Refinement Pattern\n",
        "prompt=\"\"\"Suggest me some better question patterns whenever I ask a question and ask if I would like to use it instead\n",
        "\n",
        "question=Can we watch a movie today or would a concert be better?\n",
        "answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(model=MODEL_FAST, contents=prompt)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd2a529c",
      "metadata": {
        "id": "dd2a529c"
      },
      "source": [
        "#### D- Flipped interaction Pattern\n",
        "- I would like you to ask me questions to achieve X\n",
        "- You should ask questions until condition Y is met or to achieve this goal (alternatively, forever)\n",
        "- (Optional) ask me the questions one at a time, two at a time, ask me the first question, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "cc7fb0f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "cc7fb0f9",
        "outputId": "5dc87141-69b9-4dc2-8195-2b7b40ca65e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I understand. I will ask one question at a time, and we'll see what information you share!\n\nQuestion 1: Are you currently in school, working, or retired?"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#TODO: Flipped interaction Pattern\n",
        "prompt=\"\"\" Ask me several questions to find my age. You can ask until you get relevant informations to find the answer and always one question at a time.\n",
        "Question:\n",
        "\n",
        "example:\n",
        "question:What are you studying?\n",
        "answer: Masters\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(model=MODEL_FAST, contents=prompt)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a51e8a5",
      "metadata": {
        "id": "5a51e8a5"
      },
      "source": [
        "--------------\n",
        "## 2. Prompt Methods\n",
        "#### 2.1- Zero-shot prompting\n",
        "\n",
        "**Goal:** Implement Zero Shot Prompting and print the output of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "b4e6f0d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "b4e6f0d7",
        "outputId": "a9902422-1083-4bac-b3d0-709c46791d9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay! I'll choose the numbers 15, 20, and 5.\n\n```python\n# My chosen numbers\nnum1 = 15\nnum2 = 20\nnum3 = 5\n\n# Calculate the sum\ntotal = num1 + num2 + num3\n\n# Print the output\nprint(f\"The numbers I chose are: {num1}, {num2}, and {num3}\")\nprint(f\"Their sum is: {total}\")\n```\n\n**Output:**\n\n```\nThe numbers I chose are: 15, 20, and 5\nTheir sum is: 40\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# TODO: Implement Zero Shot Prompting and print the output of the model\n",
        "prompt=\"\"\"Add any 3 numbers of you liking and print the output\n",
        "\"\"\"\n",
        "response = client.models.generate_content(model=MODEL_FAST, contents=prompt)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1827615",
      "metadata": {
        "id": "f1827615"
      },
      "source": [
        "\n",
        "#### 2.2. Few-shot prompting\n",
        "\n",
        "**Goal:** Provide **demonstrations** (input→output pairs) to steer format and tone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "48cb2013",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "48cb2013",
        "outputId": "40962b78-ab8d-4642-9e63-ecaf601fd600"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "question3:should we add sauce?\nanswer: **Absolutely! Sauce is a crucial component of almost all pasta dishes.** It adds flavor, moisture, and transforms plain pasta into a complete meal. You'll typically add your chosen sauce to the cooked and drained pasta, often mixing it together in a large pan over low heat for a minute or two to allow the flavors to meld. There's a vast array of sauces to choose from, like tomato-based (marinara, bolognese), cream-based (alfredo, carbonara), pesto, or olive oil and garlic."
          },
          "metadata": {}
        }
      ],
      "source": [
        "#TODO: Implement Few Shot Prompting and print the output of the model\n",
        "prompt=\"\"\"Give a guide to how to cook Pasta\n",
        "\n",
        "question1:how to start?\n",
        "ans:boil water first\n",
        "\n",
        "question2:should we add vegetables?\n",
        "answer: yes\n",
        "\n",
        "question3:should we add sauce?\n",
        "answer:\n",
        "\"\"\"\n",
        "response = client.models.generate_content(model=MODEL_FAST, contents=prompt)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1dc320",
      "metadata": {
        "id": "3b1dc320"
      },
      "source": [
        "\n",
        "#### 2.3. Chain-of-Thought (CoT)\n",
        "\n",
        "**Goal:** Encourage step-by-step reasoning.  \n",
        "Generate a chain of thought – series of intermediate reasoning steps- significantly improves the ability of LLMs to perform complex reasoning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "c1896a0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "c1896a0a",
        "outputId": "b378a6e0-7316-40da-d6e6-32a2f5ec2022"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's a step-by-step reasoning to determine when the two trains will meet:\n\n1.  **Calculate the head start of Train A:**\n    *   Train A leaves at 9:00 AM. Train B leaves at 10:00 AM.\n    *   This means Train A travels alone for 1 hour (from 9:00 AM to 10:00 AM).\n    *   Distance covered by Train A in this hour = Speed × Time = 60 km/h × 1 h = 60 km.\n\n2.  **Determine the remaining distance at 10:00 AM:**\n    *   The total distance between City A and City B is 420 km.\n    *   At 10:00 AM, Train A has already covered 60 km.\n    *   The remaining distance between the two trains is 420 km - 60 km = 360 km.\n\n3.  **Calculate the relative speed of the trains:**\n    *   At 10:00 AM, both trains are moving towards each other.\n    *   Relative speed = Speed of Train A + Speed of Train B\n    *   Relative speed = 60 km/h + 80 km/h = 140 km/h.\n\n4.  **Calculate the time it takes for them to meet from 10:00 AM onwards:**\n    *   Time = Remaining Distance / Relative Speed\n    *   Time = 360 km / 140 km/h\n    *   Time = 36 / 14 hours\n    *   Time = 18 / 7 hours\n\n5.  **Convert the time (18/7 hours) into hours and minutes:**\n    *   18 / 7 hours = 2 and 4/7 hours.\n    *   2 hours is straightforward.\n    *   To convert 4/7 of an hour into minutes: (4/7) × 60 minutes = 240 / 7 minutes.\n    *   240 ÷ 7 ≈ 34.28 minutes.\n    *   For practical purposes, this is approximately 2 hours and 34 minutes. (If you want to be super precise, it's 2 hours, 34 minutes, and 17 seconds).\n\n6.  **Determine the final meeting time:**\n    *   The trains started moving towards each other from 10:00 AM.\n    *   They travel for an additional 2 hours and approximately 34 minutes.\n    *   Meeting time = 10:00 AM + 2 hours 34 minutes = 12:34 PM.\n\nThe two trains will meet at **12:34 PM**."
          },
          "metadata": {}
        }
      ],
      "source": [
        "#TODO: Implement Chain of Thought Prompting and print the output of the model\n",
        "prompt=\"\"\"A train leaves City A at 9:00 AM traveling at 60 km/h. Another train leaves City B at 10:00 AM traveling towards City A at 80 km/h. The distance between City A and City B is 420 km. At what time will the two trains meet?\n",
        "\n",
        "step-by-step reasoning:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(model=MODEL_FAST, contents=prompt)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3728d90e",
      "metadata": {
        "id": "3728d90e"
      },
      "source": [
        "\n",
        "#### 2.4 Self-Consistency (sampling multiple chains)\n",
        "\n",
        "**Goal:** Improve reliability by sampling multiple reasoning paths and aggregating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "b5733baf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5733baf",
        "outputId": "adab7020-ae4c-42f0-9a46-3150453ae849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution: Counter({'Final answer: 5': 5})\n",
            "Chosen: ('Final answer: 5', 5)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import Counter\n",
        "\n",
        "def solve_with_self_consistency(prompt, n=5, temperature=1.0, model=MODEL_FAST):\n",
        "    answers = []\n",
        "    for _ in range(n):\n",
        "        resp = client.models.generate_content(\n",
        "            model=model,\n",
        "            contents=prompt,\n",
        "            config=types.GenerateContentConfig(temperature=temperature)\n",
        "        )\n",
        "        text = resp.text.strip()\n",
        "        # Naive final-answer extraction: last line or digits\n",
        "        lines = [x for x in text.splitlines() if x.strip()]\n",
        "        answers.append(lines[-1] if lines else text)\n",
        "    tally = Counter(answers)\n",
        "    return tally.most_common(1)[0], tally\n",
        "\n",
        "sc_prompt = (\n",
        "    \"Tom has 10 apples. He gives 3 to Mary and 2 to John. \"\n",
        "    \"How many apples remain? Think step by step, end with 'Final answer: <number>'.\"\n",
        ")\n",
        "\n",
        "winner, dist = solve_with_self_consistency(sc_prompt, n=5, temperature=0.9, model=MODEL_FAST)\n",
        "print('Distribution:', dist)\n",
        "print('Chosen:', winner)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59d8f59",
      "metadata": {
        "id": "a59d8f59"
      },
      "source": [
        "\n",
        "#### 2.5 Prompt chaining (sequential pipeline)\n",
        "\n",
        "**Goal:** Decompose tasks. We’ll do: Extract → Expand → Synthesize.\n",
        "- step1: Extract 3 key points from the text\n",
        "- Step2: expand each point\n",
        "- Step3 : Synthesize into one cohesive paragraph (<=120 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "639159b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "639159b9",
        "outputId": "4b93b340-e7fa-4c26-e860-d5fcaf8b75c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key points:\n",
            " Here are 3 key points from the text:\n",
            "\n",
            "1.  **Definition:** Network slicing enables the creation of multiple virtualized networks on a shared physical infrastructure.\n",
            "2.  **Purpose:** It allows resources to be tailored and optimized for specific use cases, such as eMBB, URLLC, and mMTC.\n",
            "3.  **Challenges:** Implementing network slicing involves overcoming challenges related to isolation, orchestration, and enforcing Service Level Agreements (SLAs). \n",
            "\n",
            "Expanded:\n",
            " Here are the expanded points:\n",
            "\n",
            "1.  **Definition:** Network slicing allows operators to create several independent, end-to-end logical networks atop a single, underlying physical network infrastructure. Each \"slice\" functions as a dedicated network, customized with specific resources and functionalities for different services or applications.\n",
            "2.  **Purpose:** The primary purpose of network slicing is to precisely allocate and configure network resources—such as bandwidth, latency, and reliability—to meet the diverse demands of various applications. This enables the creation of dedicated slices optimized for enhanced Mobile Broadband (eMBB) requiring high throughput, Ultra-Reliable Low-Latency Communication (URLLC) demanding extreme responsiveness, or massive Machine Type Communication (mMTC) supporting countless IoT devices.\n",
            "3.  **Challenges:** Successful deployment of network slicing faces significant hurdles, particularly in ensuring robust isolation between slices to prevent interference and maintain security. Furthermore, complex orchestration systems are required to dynamically manage and provision these slices, alongside mechanisms to effectively monitor and enforce the strict Service Level Agreements (SLAs) guaranteed to each slice. \n",
            "\n",
            "Synthesis:\n",
            " Network slicing allows operators to create multiple independent, logical networks on a single physical infrastructure, each customized with specific resources and functionalities. Its primary purpose is to precisely allocate network resources like bandwidth, latency, and reliability to meet diverse application demands, optimizing for services such as enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communication (URLLC), or massive Machine Type Communication (mMTC). However, successful deployment faces significant challenges, including ensuring robust isolation and security between slices, developing complex orchestration systems for dynamic management, and effectively monitoring and enforcing strict Service Level Agreements.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "article = \"\"\"\n",
        "Network slicing enables multiple virtualized networks on shared infrastructure.\n",
        "It tailors resources to use-cases like eMBB, URLLC, and mMTC.\n",
        "Challenges include isolation, orchestration, and SLA enforcement.\n",
        "\"\"\"\n",
        "\n",
        "p1 = client.models.generate_content(\n",
        "    model=MODEL_FAST,\n",
        "    contents=f\"Extract 3 key points from the text:\\n{article}\",\n",
        "    config=types.GenerateContentConfig(temperature=0.4)\n",
        ").text\n",
        "\n",
        "p2 = client.models.generate_content(\n",
        "    model=MODEL_FAST,\n",
        "    contents=f\"Expand each point (1-2 sentences each):\\n{p1}\",\n",
        "    config=types.GenerateContentConfig(temperature=0.6)\n",
        ").text\n",
        "\n",
        "p3 = client.models.generate_content(\n",
        "    model=MODEL_FAST,\n",
        "    contents=f\"Synthesize into one cohesive paragraph (<=120 words):\\n{p2}\",\n",
        "    config=types.GenerateContentConfig(temperature=0.5)\n",
        ").text\n",
        "\n",
        "print(\"Key points:\\n\", p1, \"\\n\\nExpanded:\\n\", p2, \"\\n\\nSynthesis:\\n\", p3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9107e49",
      "metadata": {
        "id": "f9107e49"
      },
      "source": [
        "#### 2.5.2 : Convert the **prompt chaining** into a function and add a **critique-and-revise** step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "5222fa1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5222fa1a",
        "outputId": "6328cc9c-ce5a-4bd4-daf6-9709a9af712a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2256036638.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Your input text goes here.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2256036638.py\u001b[0m in \u001b[0;36mprompt_chain\u001b[0;34m(text, client, model)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Step 4: Critique and revise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcritique_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Critique the following paragraph and revise it for clarity and conciseness:\\n{paragraph}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcritique_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcritique_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mfinal_paragraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritique_resp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5004\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5005\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5006\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5007\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5008\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3816\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3818\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   3819\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3820\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     )\n\u001b[0;32m-> 1314\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m     response_body = (\n\u001b[1;32m   1316\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1125\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       )\n\u001b[0;32m-> 1127\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m       return HttpResponse(\n\u001b[1;32m   1129\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "def prompt_chain(text, client, model=MODEL_FAST):\n",
        "    # Step 1: Extract key points\n",
        "    extract_prompt = f\"Extract 3 key points from the following text:\\n{text}\"\n",
        "    extract_resp = client.models.generate_content(model=model, contents=extract_prompt)\n",
        "    key_points = extract_resp.text.strip().split(\"\\n\")\n",
        "\n",
        "    # Step 2: Expand each point\n",
        "    expanded_points = []\n",
        "    for point in key_points:\n",
        "        expand_prompt = f\"Expand on this point in 2-3 sentences:\\n{point}\"\n",
        "        expand_resp = client.models.generate_content(model=model, contents=expand_prompt)\n",
        "        expanded_points.append(expand_resp.text.strip())\n",
        "\n",
        "    # Step 3: Synthesize into one paragraph\n",
        "    synth_prompt = \"Combine the following points into one cohesive paragraph (<=120 words):\\n\" + \"\\n\".join(expanded_points)\n",
        "    synth_resp = client.models.generate_content(model=model, contents=synth_prompt)\n",
        "    paragraph = synth_resp.text.strip()\n",
        "\n",
        "    # Step 4: Critique and revise\n",
        "    critique_prompt = f\"Critique the following paragraph and revise it for clarity and conciseness:\\n{paragraph}\"\n",
        "    critique_resp = client.models.generate_content(model=model, contents=critique_prompt)\n",
        "    final_paragraph = critique_resp.text.strip()\n",
        "\n",
        "    return final_paragraph\n",
        "\n",
        "# Usage\n",
        "text = \"Your input text goes here.\"\n",
        "result = prompt_chain(text, client)\n",
        "display(Markdown(result))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf27dba4",
      "metadata": {
        "id": "bf27dba4"
      },
      "source": [
        "\n",
        "#### 3. Structure control with JSON Response Schema\n",
        "\n",
        "**Goal:** Force well-structured, machine-parseable outputs (e.g., for pipelines).  \n",
        "We’ll ask the model to extract KPIs into a JSON object **validated by the SDK**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b734e538",
      "metadata": {
        "id": "b734e538"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class KPIReport(BaseModel):\n",
        "    kpi: str\n",
        "    value: float\n",
        "    unit: str\n",
        "    interpretation: str\n",
        "\n",
        "text = (\n",
        "    \"KPI: Latency. Value: 85 ms. In 5G eMBB, typical interactive threshold is 100 ms. \"\n",
        "    \"Provide a one-sentence interpretation.\"\n",
        ")\n",
        "\n",
        "resp = client.models.generate_content(\n",
        "    model=MODEL_FAST,\n",
        "    contents=f\"Extract a JSON report with fields (kpi, value, unit, interpretation) from: {text}\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=KPIReport,\n",
        "        temperature=0.3,\n",
        "    ),\n",
        ")\n",
        "print(resp.text)  # valid JSON per schema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39fc3582",
      "metadata": {
        "id": "39fc3582"
      },
      "source": [
        "#### give another example of structured output with a json response schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c817e323",
      "metadata": {
        "id": "c817e323"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class ArticleReport(BaseModel):\n",
        "    title: str\n",
        "    author: str\n",
        "    date: str\n",
        "    key_points: str\n",
        "    summary:str\n",
        "\n",
        "text = (\n",
        "    \"Title: AI Revolution in Healthcare. Author: Jane Doe. Date: 2025-10-23. \"\n",
        "    \"The adoption of AI in hospitals has increased efficiency and reduced errors. \"\n",
        "    \"However, ethical concerns regarding patient data privacy remain.\"\n",
        ")\n",
        "\n",
        "resp = client.models.generate_content(\n",
        "    model=MODEL_FAST,\n",
        "    contents=f\"Extract a JSON report with fields (title, author, date, key_points, summary) from: {text}\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=ArticleReport,\n",
        "        temperature=0.3,\n",
        "    ),\n",
        ")\n",
        "print(resp.text)  # valid JSON per schema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f92772b",
      "metadata": {
        "id": "5f92772b"
      },
      "source": [
        "\n",
        "#### 4. ReAct-style tool use (function calling)\n",
        "\n",
        "We’ll register a simple tool and let the model decide when to call it.  \n",
        "*(In production, you would call external APIs, search, databases, etc.)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd23046",
      "metadata": {
        "collapsed": true,
        "id": "afd23046"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Demo tool: tiny facts DB\n",
        "FACTS = {\n",
        "    \"instagram_acquirer\": \"Facebook acquired Instagram in 2012.\",\n",
        "    \"facebook_ceo\": \"The CEO of Facebook (Meta) is Mark Zuckerberg.\"\n",
        "}\n",
        "\n",
        "def lookup_fact(key: str) -> str:\n",
        "    \"\"\"Return a tiny fact from the in-memory dict.\"\"\"\n",
        "    return FACTS.get(key, \"Unknown.\")\n",
        "\n",
        "tool_config = types.ToolConfig(\n",
        "    function_calling_config=types.FunctionCallingConfig(mode=\"ANY\")\n",
        ")\n",
        "\n",
        "resp = client.models.generate_content(\n",
        "    model=MODEL_FAST,\n",
        "    contents=\"Who is the CEO of the company that acquired Instagram?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[lookup_fact],\n",
        "        tool_config=tool_config,\n",
        "        # Optional: limit number of automatic function calls\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(maximum_remote_calls=2),\n",
        "        temperature=0.3\n",
        "    ),\n",
        ")\n",
        "pp(resp)\n",
        "\n",
        "# Manual execution\n",
        "result = lookup_fact(\"facebook_ceo\")\n",
        "print(\"Result:\", result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641e2427",
      "metadata": {
        "id": "641e2427"
      },
      "source": [
        "**Extend** the **ReAct** tool set with a math `calculator(a, b, op)` function and observe function calling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed23bd7",
      "metadata": {
        "id": "eed23bd7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Generate content WITHOUT automatic function calling\n",
        "calc_resp = client.models.generate_content(\n",
        "    model=MODEL_FAST,\n",
        "    contents=\"Call the calculator function with a=12, b=7, op='*'\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[calculator],\n",
        "        temperature=0\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Parse the response manually\n",
        "if calc_resp.candidates:\n",
        "    candidate = calc_resp.candidates[0]\n",
        "\n",
        "    # Some SDK versions return function_call as a dict inside content[0].text or content[0].function_call\n",
        "    for part in candidate.content:\n",
        "        func_call = getattr(part, \"function_call\", None)\n",
        "        if func_call:\n",
        "            name = func_call.name\n",
        "            args = json.loads(func_call.arguments)\n",
        "            print(f\"Function '{name}' called with arguments {args}\")\n",
        "\n",
        "            # Execute the function\n",
        "            if name == \"calculator\":\n",
        "                result = calculator(**args)\n",
        "                print(\"Result:\", result)\n",
        "\n",
        "# Directly call the calculator function\n",
        "result = calculator(a=12, b=7, op='*')\n",
        "print(\"Result:\", result)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}